= Processamento Digita de Imagens (PDI)
Dan Morais <danvitor1997@gmail.com>
:toc: left
:icons: font
:quick-uri: http://asciidoctor.org/docs/asciidoc-syntax-quick-reference/
:source-highlighter: pygments
:stem:

Aqui serão apresentados alguns algoritmos e comentários, a respeito do processamento digital de imagens, com exemplos (exercicios) colocados pelo professor para melhor compreensão da matéria.

== Unidade 1

=== Manipulando pixels em uma imagem:

==== Exercício 1

Foi pedido para fazer um programa que ao usuário passar uma região dentro dos limites da imagem o algoritmo deve negativar a área.

O código pode ser visto a seguir:

[[app-listing]]
[source, cpp]
.Negativo
----
#include <iostream>
#include <cv.h>
#include <highgui.h>
#include <opencv2/opencv.hpp>

using namespace cv;
using namespace std;

int main(int, char**){
  Mat image;

  int width, height;
  Vec4i pontos; //vetor para os pontos P1 e P2

  int Cpixel;

  image= imread("mario.png",CV_LOAD_IMAGE_GRAYSCALE);
  if(!image.data)
    cout << "falha ao abrir a imagem" << endl;

//pega o tamanho da imagem, em pixels
  width=image.size().width;
  height=image.size().height;

  namedWindow("janela",WINDOW_AUTOSIZE);

  cout << "Escolha dois pontos (x,y) da imagem, levando em consideração que a imagem é de tamanho " << width << "x" << height << "." << endl;

  cout << "Ponto P1:" << endl << "x = ";
  cin >> pontos[0];
  cout << "y = ";
  cin >> pontos[1];

  cout << "Ponto P2:" << endl << "x = ";
  cin >> pontos[2];
  cout << "y = ";
  cin >> pontos[3];

//condições de "tamanho" das coordenadas, para que o loops funcionem

  if((pontos[2]>pontos[0])&(pontos[3]>pontos[1])){
    for(int i=pontos[0];i<pontos[2];i++){
      for(int j=pontos[1];j<pontos[3];j++){
        Cpixel = image.at<uchar>(i,j); //pega a cor atual do pixel em (i,j)
        image.at<uchar>(i,j)= 255 - Cpixel; //negativo
      }
    }
  } else if((pontos[0]>pontos[2])&(pontos[3]>pontos[1])){
    for(int i=pontos[2];i<pontos[0];i++){
      for(int j=pontos[1];j<pontos[3];j++){
        Cpixel = image.at<uchar>(i,j);
        image.at<uchar>(i,j)= 255 - Cpixel;
      }
    }
  } else if((pontos[2]>pontos[0])&(pontos[1]>pontos[3])){
    for(int i=pontos[0];i<pontos[2];i++){
      for(int j=pontos[3];j<pontos[1];j++){
        Cpixel = image.at<uchar>(i,j);
        image.at<uchar>(i,j)= 255 - Cpixel;
      }
    }
  } else {
    for(int i=pontos[2];i<pontos[0];i++){
      for(int j=pontos[3];j<pontos[1];j++){
        Cpixel = image.at<uchar>(i,j);
        image.at<uchar>(i,j)= 255 - Cpixel;
      }
    }
  }
  imshow("janela", image);
  imwrite("negativo.png", image);
  waitKey();

  return 0;
}
----


Neste Código chamamos uma imagem e pegamos seu tamanho (altura e largura), a partir dai criamos condições de tamanho para que o loop possa vir a funcionar, uma vez funcionando subtraimos neste loop 255 da cor do pixel atual, desta maneira temos o valor negativado. Ao termino do loop teremos a região informada pelo usuário totalmente negativada como podemos observar na figura a segir:

[[img-negativo]] 
.negativo
image::negativo.png[]


==== Exercício 2
Aqui foi proposto para que nós fizessemos um algoritmo que realizaria trocas aleatorias de 4 regiões da imagem, porém em sala o professor pediu para que trocassemos em diagonais, pois utilizariamos tal conceito mais a frente.

Com isso temos este código:

[[app-listing]]
[source, cpp]
.TrocarRegioes.cpp
----
#include <iostream>
#include <highgui.h>
#include <opencv2/opencv.hpp>

using namespace cv;
using namespace std;

int main(int qtdArg, char** args) {
    Mat image;

    image = imread("mario.png",CV_LOAD_IMAGE_GRAYSCALE);
    int width = image.size().width;
    int height = image.size().height;

    //Mapeando Rect
    Mat X(image, Rect(0, 0, width/2, height/2));
    Mat W(image, Rect(width/2, 0, width/2, height/2));
    Mat Y(image, Rect(0, height/2, width/2, height/2));
    Mat Z(image, Rect(width/2, height/2, width/2, height/2));

    //Cria nova Mat
    Mat saida = Mat::zeros(image.size(), image.type());
    Mat aux;

    //MApeia nova região da matriz de saída com um auxiliar 
    aux = saida.colRange(0, width/2).rowRange(0, height/2);
    //Copia o conteúdo
    Z.clone().copyTo(aux);

    aux = saida.colRange(width/2, width).rowRange(0, height/2);
    Y.copyTo(aux);

    aux = saida.colRange(0, width/2).rowRange(height/2, height);
    W.copyTo(aux);

    aux = saida.colRange(width/2, width).rowRange(height/2, height);
    X.copyTo(aux);

    if(!image.data)
        cout << "falha ao abrir a imagem biel.png" << endl;

    namedWindow("janela", WINDOW_AUTOSIZE);
    imshow("janela", saida);
    imwrite("troca.png", saida);
    waitKey();
    return 0;

}
----


Neste algoritmo, mais uma vez lemos a imagem e pegamos seu tamanho (largura e altura), com estas informações mapeamos toda imagem e a dividimos em 4 matrizes (pegamos metade da altura e metade da largura para essa divisão). Depois criamos uma nova matriz e com a ajuda de um auxiliar (aux) colocamos as 4 partes em seus devidos locais a *Primeira* trocamos com a *Quarta* matriz, e a *Segunda* com a *Terceira* matriz.

Segue Resultado:

[[img-troca]] 
.Troca
image::troca.png[]

=== Preenchendo regiões:

==== Exercício 1

Aqui nos é pedido para que identifiquemos o prolema que teria em usar a rotulação e uma solução para ele. O problema seria dado que se uma imagem tivesse mais de 255 objetos, não teriamos mais como routulalos ja que so temos 255 tons. A solução pode ser dada em ao chegar no 255, manter o mesmo rotulo para todo resto de objetos identificados, porem ainda assim acrescentando no contador de objetos.

==== Exercício 2

Aqui nos é pedido para que aprimoremos o algoritmo de contagem para que possamos contar somente objetos sólidos (não contém bolhas). Lembrando que os objetos na borda não podemos afirmar se são sólidos ou não e com isso devemos descarta-los.

Esta é a imagem a qual deveriamos contar o numero de objetos que nao continham bolhas:

[[img-bolhas]] 
.Bolhas
image::bolhas.png[]


O codigo é:

[[app-listing]]
[source, cpp]
.SoSolidos.cpp
----
#include <iostream>
#include <opencv2/opencv.hpp>

using namespace cv;

int main(int argc, char** argv){
  Mat image, mask;
  int width, height;
  int C_Bolha = 67;
  int S_Bolha = 196;
  CvPoint p;
  p.x = 0;
  p.y = 0;
  image = imread(argv[1],CV_LOAD_IMAGE_GRAYSCALE);

  if(!image.data){
    std::cout << "Falha ao abrir imagem\n";
    return(-1);
  }
  imshow("imageOriginal", image);
  width = image.size().width;
  height = image.size().height;

  //Pintar de branco os elementos da borda 
  for (int i = 0; i < width; i++) {
    image.at<uchar>(i, height -1) = 255;
    image.at<uchar>(i, 0) = 255;
  }

  for (int i = 0; i < height; i++) {
    image.at<uchar>(0, i) = 255;
    image.at<uchar>(width -1, i) = 255;
  }
//Elimina todos elementos que tocam as bordas
  floodFill(image, p, 0);

//Definicao do "fundo da imagem"
  floodFill(image, p, 1);


  //Procurando todos elementos
  int qtdTotal = 0;
  for (int i = 0; i < height; i++) {
    for (int j = 0; j < width; j++) {
      if (image.at<uchar>(i, j) == 255) {
        p.x = j;
        p.y = i;
        floodFill(image, p, S_Bolha);
        qtdTotal++;
      }
    }
  }

  //Procurando apenas elementos que possuam bolhas
  int qtdComBolhas = 0;
  for (int i = 0; i < height; i++) {
    for (int j = 0; j < width; j++) {
      //Armazena a posição do elemento encontrado, independente se for rotulado com bolha ou sem bolha
      if (image.at<uchar>(i,j) == S_Bolha || image.at<uchar>(i,j) == C_Bolha) {
        p.x = j;
        p.y = i;
      } else if (image.at<uchar>(i,j) == 0) {
        //Caso for encontrado um buraco, verficar se ja foi rotulado, caso contrario rotule o ultimo elemento encontrado como "C_Bolha"
        if (image.at<uchar>(p.y, p.x) == S_Bolha) {
          floodFill(image, p, C_Bolha);
          qtdComBolhas++;
        }
        //Rotule o buraco encontrado como C_Bolha
        p.x = j;
        p.y = i;
        floodFill(image, p, C_Bolha);
      }
    }
  }

  imshow("image", image);
  imwrite("labeling.png", image);
  printf("elementos sem bolhas: %d, com bolhas: %d\n", qtdTotal - qtdComBolhas, qtdComBolhas);
  waitKey();
  return 0;
}
----

Neste código ao adquirirmos a imagem identificamos objetos que toquem a borda e o retiramos com um *floodfill*, após isso mudamos a cor do plano de fundo para que o fundo seja diferenciado da bolha do objeto. Com isso percorremos toda a imagem procurando os objetos que são sólidos.

[[img-sosolidos]] 
.Só Solidos
image::sosolidos.png[]

=== Manipulação de histogramas:

==== Exercício 1

Nos foi pedido para que com base no *histogram.cpp* fizessemos um progrma chamado *equalizer.cpp* que para cada imagem captura realizasse a equalização do histograma antes que viesse a exibir a imagem.

Código:

[[app-listing]]
[source, cpp]
.equalizer.cpp
----
#include <iostream>
#include <opencv2/opencv.hpp>

using namespace cv;
using namespace std;

int main(int argc, char** argv){
  Mat image, eqImg;
  int width, height;
  VideoCapture cap;
  vector<Mat> planes, planesEq;
  Mat histR, histG, histB;
  int nbins = 64;
  float range[] = {0, 256};
  const float *histrange = { range };
  bool uniform = true;
  bool acummulate = false;

  cap.open(0);

  if(!cap.isOpened()){
    cout << "cameras indisponiveis";
    return -1;
  }

  width  = cap.get(CV_CAP_PROP_FRAME_WIDTH);
  height = cap.get(CV_CAP_PROP_FRAME_HEIGHT);

  //cap.set(CV_CAP_PROP_POS_FRAMES,100);

  cout << "largura = " << width << endl;
  cout << "altura  = " << height << endl;

  int histw = nbins, histh = nbins/2;
  Mat histImgR(histh, histw, CV_8UC3, Scalar(0,0,0));
  Mat histImgG(histh, histw, CV_8UC3, Scalar(0,0,0));
  Mat histImgB(histh, histw, CV_8UC3, Scalar(0,0,0));

  while(1){
    cap >> image;
	flip(image, image, 1);

	//--etapa 1-- cria nova imagem(eqimg), equaliza e exibe
    cvtColor(image, eqImg, CV_BGR2YCrCb); //muda o formato de cor da imagem para YCrCb
    split(eqImg,planesEq);
    equalizeHist(planesEq[0], planesEq[0]); //equaliza histograma para o primeiro canal Y
    merge(planesEq, eqImg); //junta os planos incluindo o plano equalizado(intensidade de luz)
    cvtColor(eqImg, eqImg, CV_YCrCb2BGR); //modifica o formato de cores para RGB novamente para exibição
    //--FIM etapa1--

    split (image, planes);
    calcHist(&planes[0], 1, 0, Mat(), histR, 1,
             &nbins, &histrange,
             uniform, acummulate);


    normalize(histR, histR, 0, histImgR.rows, NORM_MINMAX, -1, Mat());
    histImgR.setTo(Scalar(0));


    for(int i=0; i<nbins; i++){
      line(histImgR, Point(i, histh),
           Point(i, cvRound(histR.at<float>(i))),
           Scalar(0, 0, 255), 1, 8, 0);

    }
    histImgR.copyTo(image(Rect(0, 0       ,nbins, histh)));


	namedWindow("Original Image", CV_WINDOW_AUTOSIZE);
	namedWindow("Equalized Image", CV_WINDOW_AUTOSIZE);

    imshow("Original Image", image);
	imshow("Equalized Image", eqImg);

    if(waitKey(30) >= 0) break;
  }
  return 0;
}
----

Neste código foi utilizado uma função do opencv para fazer o calculo do histograma. O intuido deste calculo é para a distribuição dos pixels fique uniforme, desta maneira temos uma melhor distribuição de brilo na imagem. Este processo é fácil ser notado em uma imagem em _Grayscale_, porém para imagens coloridas não temos a mesma facilidade até porque não é correto aplicar uma equalização num RGB, desta maneira convertemos a imagem colorida para YCrCb para que possamos separar a cor da intensidade luminosa da imagem como é possivel observar nas imagens abaixo:

[[img-ImgEQ]] 
.Imagem Equalizada
image::ImgEQ.png[]

==== Exercício 2

Aqui foi pedido para que fizessemos um algoritmo chamado *motiondetector.cpp* onde seria feito o calculo contínuo dos histogramas da imagem e quando a diferença entre os histogramas passasse um determinado limite, o algoritmo deverias identificar que houve movimento.

Temos portanto este codigo:

[[app-listing]]
[source, cpp]
.motiondetector.cpp
----
#include <iostream>
#include <opencv2/opencv.hpp>

using namespace cv;
using namespace std;

double tolerancia = 0.995;
int COUNT_MAX = 3;
bool uniform = true;
bool acummulate = false;
int nbins = 64;
float range[] = {0, 256};
const float *histrange = { range };

void calcHistogram(Mat *out, Mat image) {


    vector<Mat> planes;
    split (image, planes);
    calcHist(&planes[0], 1, 0, Mat(), *out, 1,
             &nbins, &histrange,
             uniform, acummulate);
    normalize(*out, *out, 0, 1, NORM_MINMAX, -1, Mat());
}

int main(int argc, char** argv){
    Mat image;
    int width, height;
    VideoCapture cap;
    Mat histR;
    cap.open(0);

    if(!cap.isOpened()){
        cout << "cameras indisponiveis";
        return -1;
    }

    width  = cap.get(CV_CAP_PROP_FRAME_WIDTH);
    height = cap.get(CV_CAP_PROP_FRAME_HEIGHT);

    cout << "largura = " << width << endl;
    cout << "altura  = " << height << endl;

    int histw = nbins, histh = nbins/2;
    Mat lastHist;
    cap >> lastHist;
    calcHistogram(&lastHist, lastHist);
    double correlacao = 1;
    int cont = 0;
    while(1){
        cap >> image;

        calcHistogram(&histR, image);

        correlacao = compareHist(histR, lastHist, CV_COMP_CORREL);
        lastHist = histR.clone();

        if (correlacao < tolerancia) {
            cont++;
            if (cont == COUNT_MAX) {
                circle(image, Point(16, 16), 15, Scalar(0, 0, 255), CV_FILLED);
                putText(image, "MEXEU!!", cvPoint(120,45), FONT_HERSHEY_COMPLEX_SMALL, 0.8, cvScalar(0,0,250), 1, CV_AA);
            }
        } else {
            cont = 0;
        }
        imshow("image", image);
        if(waitKey(30) >= 0) break;
    }
    return 0;
}
----


Para este algoritmo teriamos um calculo continuo de histogramas, um limite para variações, e um alerta a ser emitido. 

Com base na camera disponivel calculamos um limite para que nao haja alertas desnecessários, porém ainda assim possamos identificar quando essa variação venha ocorrer. Uma vez que possuimos esse valor de limite o utilizamos para que todas as vezes que o histograma tenha uma variação que seja maior que este limite emita - se um alerta (para meu caso uma bola vermelha junto com a indicação de "MEXEU"). Observe abaixo:

[[img-motiondetector]] 
.Motion Detector
image::motiondetector.png[]

=== Filtragem no domínio espacial I

==== Exercício 1

Aqui foi pedido utilizando como base o algoritmo *filtroespacial.cpp* fizessemos um algoritmo chamado de *lapgauss.cpp* usando o filtro laplaciano e o filtro gaussiano e comparar com a utilização do filtro laplaciano.

Neste Código tudo que tinhamos que fazer era uma função que passasse o filto gaussiano antes da aplicação do filtro laplaciano. Com isso temos:

[[app-listing]]
[source, cpp]
.lapgauss.cpp
----
#include <iostream>
#include <opencv2/opencv.hpp>

using namespace cv;
using namespace std;

void printmask(Mat &m){
  for(int i=0; i<m.size().height; i++){
    for(int j=0; j<m.size().width; j++){
      cout << m.at<float>(i,j) << ",";
    }
    cout << endl;
  }
}

void menu(){
  cout << "\npressione a tecla para ativar o filtro: \n"
	"a - calcular modulo\n"
    "m - media\n"
    "g - gauss\n"
    "v - vertical\n"
	"h - horizontal\n"
    "l - laplaciano\n"
    "s - laplaciano do gaussiano\n"
	"esc - sair\n";
}

int main(int argvc, char** argv){
  VideoCapture video;
  float media[] = {1,1,1,
				   1,1,1,
				   1,1,1};
  float gauss[] = {1,2,1,
				   2,4,2,
				   1,2,1};
  float horizontal[]={-1,0,1,
					  -2,0,2,
					  -1,0,1};
  float vertical[]={-1,-2,-1,
					0,0,0,
					1,2,1};
  float laplacian[]={0,-1,0,
					 -1,4,-1,
					 0,-1,0};

  Mat cap, frame, frame32f, frameFiltered;
  Mat mask(3,3,CV_32F), mask1;
  Mat result, result1;
  double width, height, min, max;
  int absolut;
  char key;
  
  video.open(0); 
  if(!video.isOpened()) 
    return -1;
  width=video.get(CV_CAP_PROP_FRAME_WIDTH);
  height=video.get(CV_CAP_PROP_FRAME_HEIGHT);
  std::cout << "largura=" << width << "\n";;
  std::cout << "altura =" << height<< "\n";;

  namedWindow("filtroespacial",1);

  mask = Mat(3, 3, CV_32F, media); 
  scaleAdd(mask, 1/9.0, Mat::zeros(3,3,CV_32F), mask1);
  swap(mask, mask1);
  absolut=1; // calcs abs of the image

  menu();
  for(;;){
    video >> cap; 
    cvtColor(cap, frame, CV_BGR2GRAY);
    flip(frame, frame, 1);
    imshow("original", frame);
    frame.convertTo(frame32f, CV_32F);
    filter2D(frame32f, frameFiltered, frame32f.depth(), mask, Point(1,1), 0);
    if(absolut){
      frameFiltered=abs(frameFiltered);
    }
    frameFiltered.convertTo(result, CV_8U);
    imshow("filtroespacial", result);
    key = (char) waitKey(10);
    if( key == 27 ) break; // esc pressed!
    switch(key){
    case 'a':
	  menu();
      absolut=!absolut;
      break;
    case 'm':
	  menu();
      mask = Mat(3, 3, CV_32F, media);
      scaleAdd(mask, 1/9.0, Mat::zeros(3,3,CV_32F), mask1);
      mask = mask1;
      printmask(mask);
      break;
    case 'g':
	  menu();
      mask = Mat(3, 3, CV_32F, gauss);
      scaleAdd(mask, 1/16.0, Mat::zeros(3,3,CV_32F), mask1);
      mask = mask1;
      printmask(mask);
      break;
    case 'h':
	  menu();
      mask = Mat(3, 3, CV_32F, horizontal);
      printmask(mask);
      break;
    case 'v':
	  menu();
      mask = Mat(3, 3, CV_32F, vertical);
      printmask(mask);
      break;
    case 'l':
	  menu();
      mask = Mat(3, 3, CV_32F, laplacian);
      printmask(mask);
      break;
        case 's':
    menu();
      mask = Mat(3, 3, CV_32F, gauss);
      printmask(mask);
      mask = Mat(3, 3, CV_32F, laplacian);
      printmask(mask);
      break;
    default:
      break;
    }

  }
  return 0;
}
----

O que observamos neste algoritmo é que ele melhora a identificação de bordas da imagem, pois com o gaussiano diminuimos um pouco o ruido da imagem e assim facilitamos a identificação das bordas pelo laplaciano.

Aqui está a imagem do laplaciano:

[[img-laplaciano]] 
.Laplaciano
image::laplaciano.png[]

Agora observemos o laplaciano do gaussiano:

[[img-lapgauss]] 
.Laplaciano do Gaussiano
image::lapgauss.png[]


=== Filtragem no domínio espacial II

==== Exercício 1

Neste exercicio o professor pediu para que produzissemos um filtro _TiltShift_ com regulação de altura da região, força do decaimento e ajuste para regular a posição vertical da imagem.



Temos primeiro a criação de sliders como o professor exigiu uma vez criados e identificados selecionamos uma imagem para trabalhar e criamos uma mascara para borramento da imagem, com isso criado vamos ter duas imagens uma onde vemos em uma o borramento e em outra a imagem na de borramentos deixamos as bordas com maior ocorrência e o meio com menor (borda = 1 meio = 0) e nada imagem o contrário bordas com menor ocorrencia e meio com maior (bordas = 0, meio = 1) e logo após isso somamos os dois resultados e temos o filtro tilt shift. O código é apresentado abaixo

[[app-listing]]
[source, cpp]
.tiltshift.cpp
----
#include <iostream>
#include <opencv2/opencv.hpp>

using namespace cv;
using namespace std;

int height;

int MAX = 100;
double decaimento = 6;
int decaimento_slider = 0;

int top_slider = 0;
int vertical = 0;

int faixa = 20;
int altura_slider = 0;


Mat image1, image2, alpha, beta;

char TrackbarName[50];

void Calc(){
  Mat ponderada1, ponderada2, tiltshift;
  int l1 = - faixa/2;
    int l2 = -l1;
    alpha = Mat::zeros(image1.rows, image1.cols, CV_32F);
    beta = Mat::zeros(image1.rows, image1.cols, CV_32F);
    int i, j;
    for (i = 0; i < alpha.rows; i++) {
        int x = i - (vertical + faixa/2);
        float alphaValue = 0.5f * (tanh((x - l1)/decaimento) - tanh((x - l2)/decaimento));
        for (j = 0; j < alpha.cols; j++) {
            alpha.at<float>(i, j) = alphaValue;
            beta.at<float>(i, j) = 1 - alphaValue;
        }
    }
    Mat auxA[] = {alpha, alpha, alpha};
    Mat auxB[] = {beta, beta, beta};
    merge(auxA, 3, alpha);
    merge(auxB, 3, beta);

  image1.convertTo(ponderada1, CV_32FC3);
    image2.convertTo(ponderada2, CV_32FC3);

    multiply(ponderada1, alpha, ponderada1);
    multiply(ponderada2, beta, ponderada2);

    add(ponderada1, ponderada2, tiltshift);
    tiltshift.convertTo(tiltshift, CV_8UC3);
    imshow("tiltshift", tiltshift);

    imshow("imagem", alpha);
  imshow("borramento", beta);
}

void on_trackbar_decaimento(int, void*){
    decaimento = (double) decaimento_slider;
    if (decaimento < 1) {
        decaimento = 1;
    }
    Calc();
}

void on_trackbar_vertical(int, void *){
    vertical = top_slider*height/MAX;
    Calc();
}

void on_trackbar_altura(int, void*) {
    faixa = altura_slider*height/MAX;
    if (faixa == 0) {
        faixa = 1;
    }

    if (faixa > height) {
        faixa = height;
    }
    Calc();
}

int main(int argvc, char** argv){
  image1 = imread("cidade.png");
  height = image1.size().height;
  image2 = image1.clone();

  Mat aux, mask, mask1;
    float media[] = {1,1,1,
                     1,1,1,
                     1,1,1};
    //para efeito de borramento
    mask = Mat(3, 3, CV_32F, media);
    scaleAdd(mask, 1/9.0, Mat::zeros(3,3,CV_32F), mask1);
    mask = mask1;
    image2.convertTo(aux, CV_32F);
    for (int i = 0; i < 10; i++) {
        filter2D(aux, aux, aux.depth(), mask, Point(1, 1), 0);
    }
    aux=abs(aux);
    aux.convertTo(image2, CV_8UC3);

    namedWindow("tiltshift", 1);

    sprintf( TrackbarName, "Altura da região");
    createTrackbar( TrackbarName, "tiltshift",
                    &altura_slider,
                    MAX,
                    on_trackbar_altura);

    sprintf( TrackbarName, "Decaimento");
    createTrackbar( TrackbarName, "tiltshift",
                    &decaimento_slider,
                    MAX,
                    on_trackbar_decaimento);

    sprintf( TrackbarName, "Posição Vertical");
    createTrackbar( TrackbarName, "tiltshift",
                    &top_slider,
                    MAX,
                    on_trackbar_vertical );
    Calc();

  waitKey(0);
  return 0;
}
----

[[img-detalhamento]] 
.Detalhamento
image::detalhamento.png[]

A imagem acima nos mostra o que foi comentado sobre maiores e menores ocorrencias da imagem e do borramento, ja abaixo podemos ver a imagem original e o resultado do programa:

[[img-cidade]] 
.Cidade
image::cidade.png[]


[[img-tiltshift]] 
.TiltShift
image::tiltshift.png[]

== Unidade 2

=== Filtragem no domínio da frequência

==== Exercício 1

Neste Exercício foi sugerido pra que usassemos o programa do professor como base para implementação do algoritmo do filtro homomórfico, este filtro tem como objetivo melhorar (homogenizar) a iluminação do ambiente, que foi mal iluminada.

Para fazer este código devemos ter algumas coisas em mente, a primeira delas é que vamos trabalhar no dominio da frequência, isso ocorrerá para que desta maneira venhamos aproveitar senoides de altas e baixas frequências. Segundo devemos saber que a má iluminação tem como resultado uma alteração de baixa de frequência, isso vai ocorrer, pois a variação de luminosidade é pequena por pixel, então ao filtrarmos essa imagem devemos deixar passar somente a alta frequência e filtrar a baixa frequência. Desta maneira vamos utilizar dos componentes da imagem (ruidos, reflectância ...) no dominio da frequência de tal modo que eles estarão se mutiplicando e nós precisamos deles sendo somados para que ocorra a filtragem de maneira correta, para isto aplicamos o logaritmo fazemos a filtragem eliminando as baixas frequências uma vez filtrado aplicamos o exponêncial e logo após a transformada reversa.

Segue Código:

[[app-listing]]
[source, cpp]
.FiltroHomomorfico.cpp
----
#include <iostream>
#include <opencv2/opencv.hpp>
#include <math.h>

using namespace cv;
using namespace std;

int height;

int dft_M, dft_N;

float MAX = 100.0;
float gamal = 0;
float max_gamal = 100;
int gamal_slider = 0;

int d0_slider = 0;
float max_d0 = 256;
float d0 = 0;

float gamah = 0;
float max_gamah = 100;
int gamah_slider = 0;


Mat image, imageDft, padded;

char TrackbarName[50];

// troca os quadrantes da imagem da DFT como havia sido feito na 1 unidade
void deslocaDFT(Mat& image ){
    Mat aux, A, B, C, D;

    // se impar recorta para nao ter imagens desiguais
    image = image(Rect(0, 0, image.cols & -2, image.rows & -2));
    int cx = image.cols/2;
    int cy = image.rows/2;

    // reorganiza os quadrantes da transformada
    A = image(Rect(0, 0, cx, cy));
    B = image(Rect(cx, 0, cx, cy));
    C = image(Rect(0, cy, cx, cy));
    D = image(Rect(cx, cy, cx, cy));

    // A <-> D
    A.copyTo(aux);  D.copyTo(A);  aux.copyTo(D);

    // C <-> B
    C.copyTo(aux);  B.copyTo(C);  aux.copyTo(B);
}

void calcFiltroHomomorfico() {
    Mat filter = Mat(padded.size(), CV_32FC2, Scalar(0));
    Mat aux = Mat(dft_M, dft_N, CV_32F);

    for (int i = 0; i < dft_M; i++) {
        for (int j = 0; j < dft_N; j++) {
            float d2 = pow(i - dft_M/2.0, 2) + pow(j - dft_N/2.0, 2);
            float exp = - (d2/pow(d0, 2));
            float valor = (gamah - gamal)*(1 - expf(exp) ) + gamal;
            aux.at<float> (i,j) = valor;
        }
    }

    Mat comps[] = {aux, aux};
    merge(comps, 2, filter);

    Mat dftClone = imageDft.clone();

    mulSpectrums(dftClone,filter,dftClone,0);

    deslocaDFT(dftClone);

    idft(dftClone, dftClone);

    vector<Mat> planos;

    split (dftClone, planos);

    normalize(planos[0], planos[0], 0, 1, CV_MINMAX);

    char bufferd0[20], bufferyh[20], bufferyl[20];
    sprintf(bufferd0, "D0: %f", d0);
    sprintf(bufferyh, "Gama H: %f", gamah);
    sprintf(bufferyl, "Gama L: %f", gamal);
    putText(planos[0], bufferd0,cv::Point(0,10), CV_FONT_HERSHEY_SIMPLEX, 0.35, cv::Scalar(255),1,8,false);
    putText(planos[0], bufferyh,cv::Point(0,20), CV_FONT_HERSHEY_SIMPLEX, 0.35, cv::Scalar(255),1,8,false);
    putText(planos[0], bufferyl,cv::Point(0,30), CV_FONT_HERSHEY_SIMPLEX, 0.35, cv::Scalar(255),1,8,false);
    imshow("Filtro Homomorfico", planos[0]);
    imshow("Original", image);
}

void on_trackbar_gamal(int, void*){
    gamal = (float) gamal_slider;
    gamal = max_gamal*gamal/MAX;
    calcFiltroHomomorfico();
}

void on_trackbar_d0(int, void *){
    d0 = d0_slider*max_d0/MAX;
    calcFiltroHomomorfico();
}

void on_trackbar_gamah(int, void*) {
    gamah = gamah_slider*max_gamah/MAX;
    calcFiltroHomomorfico();
}

int main(int argvc, char** argv){
    image = imread("homomorfica.png");
    cvtColor(image, image, CV_BGR2GRAY);

    height = image.size().height;

    // identifica os tamanhos otimos para
    // calculo do FFT
    dft_M = getOptimalDFTSize(image.rows);
    dft_N = getOptimalDFTSize(image.cols);

    // realiza o padding da imagem
    Mat_<float> zeros;
    copyMakeBorder(image, padded, 0,
                   dft_M - image.rows, 0,
                   dft_N - image.cols,
                   BORDER_CONSTANT, Scalar::all(0));

    // parte imaginaria da matriz complexa (preenchida com zeros)
    zeros = Mat_<float>::zeros(padded.size());

    // prepara a matriz complexa para ser preenchida
    imageDft = Mat(padded.size(), CV_32FC2, Scalar(0));

    copyMakeBorder(image, padded, 0,
                   dft_M - image.rows, 0,
                   dft_N - image.cols,
                   BORDER_CONSTANT, Scalar::all(0));

    Mat_<float> realInput = Mat_<float>(padded);

    // insere as duas componentes no array de matrizes
    vector<Mat> planos;
    planos.push_back(realInput);
    planos.push_back(zeros);

    // combina o array de matrizes em uma unica
    // componente complexa
    merge(planos, imageDft);

    // calcula o dft
    dft(imageDft, imageDft);
    deslocaDFT(imageDft);

    namedWindow("Filtro Homomorfico", 1);

    sprintf( TrackbarName, "gama H");
    createTrackbar( TrackbarName, "Filtro Homomorfico",
                    &gamah_slider,
                    MAX,
                    on_trackbar_gamah);

    sprintf( TrackbarName, "gama L");
    createTrackbar( TrackbarName, "Filtro Homomorfico",
                    &gamal_slider,
                    MAX,
                    on_trackbar_gamal);

    sprintf( TrackbarName, "D0");
    createTrackbar( TrackbarName, "Filtro Homomorfico",
                    &d0_slider,
                    MAX,
                    on_trackbar_d0 );
    calcFiltroHomomorfico();
    waitKey(0);

    return 0;
}
----

Para este código vemos algumas funções implementadas pelo onpenCV como a DFT, Padding e outras, o código consiste na realização da transformada de fourrier da matriz de pixel da imagens, depois realizamos a troca de regiões como foi efetuado na primeira unidade. Construimos a matriz do filtro utilizando a formula:

stem:[ H(u,v) = \gamma H - \gamma L * 1 - exp((-c*D²(u,v))/(D0)²) + \gamma L ]

Mutiplicamos H(u,v) pela matriz imagem e obtemos uma nova matriz, nessa nova matriz trocamos os quadrantes de novo e passamos a transformada inversa de fourrier. A imagem obtida será a imagem filtrada. 

Para este exercício utilizei a seguinte imagem:

[[img-homomorfica]] 
.Homomórfica
image::homomorfica.png[]

Desta imagem aplicou-se o código acima e obtive como resultado:

[[img-FiltroH]] 
.Filtrada
image::FiltroH.png[]

Desta maneira podemos observar que a luz está de maneira mais homogênea na imagem.

=== Canny e a arte com pontilhismo

==== Exercício 1

Neste exercício foi pedido para utilizarmos o algoritmo de detecção de bordas para que melhorassemos a tecnica do pontilhismo utilizando-se dos códigos disponibilizados pelo professor de pontilhimso de canny.

Segue Código:

[[app-listing]]
[source, cpp]
.cannypoints.cpp
----
#include <iostream>
#include "opencv2/opencv.hpp"

using namespace std;
using namespace cv;

int top_slider = 70;
int top_slider_max = 200;

int raio_slider = 5;
int jitter = 3;
int step = 5;

int width, height;

struct Ponto {
    int x, y;
};

#define RAIO_MAX 10
#define JITTER_MAX 10
#define STEP_MAX 20

char TrackbarName[50];

Mat image, border, points;

void calcPontilhismo() {
  vector<int> yrange;
  vector<int> xrange;
  int width, height;
  int x, y;

  width=image.size().width;
  height=image.size().height;

  xrange.resize(height/step);
  yrange.resize(width/step);

  iota(xrange.begin(), xrange.end(), 0);
  iota(yrange.begin(), yrange.end(), 0);

  for(uint i=0; i<xrange.size(); i++){
    xrange[i]= xrange[i]*step+step/2;
  }

  for(uint i=0; i<yrange.size(); i++){
    yrange[i]= yrange[i]*step+step/2;
  }


  points = Mat(height, width, CV_8U, Scalar(255, 255, 255));

  random_shuffle(xrange.begin(), xrange.end());


  uchar cor;

  for(auto i : xrange){
    random_shuffle(yrange.begin(), yrange.end());
    for(auto j : yrange){
      x = i+rand()%(2*jitter)-jitter+1;
      y = j+rand()%(2*jitter)-jitter+1;
      cor = image.at<uchar>(x,y);
      circle(points,
             cv::Point(y,x),
             raio_slider,
             Scalar(cor,cor,cor),
             -1,
             CV_AA);
    }
  }


  vector<vector<Point>> contornos;
  vector<Vec4i> hierarquia;
  findContours(border, contornos, hierarquia, CV_RETR_TREE, CV_CHAIN_APPROX_NONE);
  for (int i = 0; i < contornos.size(); i++){
    for (int j = 0; j < contornos[i].size(); j++) {
      uchar cor = image.at<uchar>(contornos[i][j].y, contornos[i][j].x);
      circle(points, cv::Point(contornos[i][j].x, contornos[i][j].y),
      1,
      CV_RGB(cor, cor, cor),
      -1,
      CV_AA);
    }
  }

  imshow("cannypoints", points);


}

void on_trackbar_pontilhismo(int, void*) {

  if (jitter < 1) jitter = 1;
  if (raio_slider < 1) raio_slider = 1;
  if (step < 1) step = 1;
  calcPontilhismo();
  imshow("canny", border);

}

void on_trackbar_canny(int, void*){
  Canny(image, border, top_slider, 3*top_slider);
  on_trackbar_pontilhismo(top_slider, 0);

}

int main(int argc, char**argv){

  image= imread(argv[1],CV_LOAD_IMAGE_GRAYSCALE);
  
  width=image.size().width;
  height=image.size().height;

  sprintf( TrackbarName, "Threshold Canny");

  namedWindow("cannypoints",1);
  createTrackbar( TrackbarName, "cannypoints",
                &top_slider,
                top_slider_max,
                on_trackbar_canny );

  sprintf( TrackbarName, "Raio");
  createTrackbar( TrackbarName, "cannypoints",
                  &raio_slider,
                  RAIO_MAX,
                  on_trackbar_pontilhismo );

  sprintf( TrackbarName, "Jitter");
  createTrackbar( TrackbarName, "cannypoints",
                  &jitter,
                  JITTER_MAX,
                  on_trackbar_pontilhismo );

  sprintf( TrackbarName, "Step");
  createTrackbar( TrackbarName, "cannypoints",
                  &step,
                  STEP_MAX,
                  on_trackbar_pontilhismo );

  on_trackbar_canny(top_slider, 0 );

  waitKey();
  return 0;
}
----


Como foi deixado livre a imagem e a forma, preferi fazer por sliders para que pudesse "brincar" com a imagem de várias maneiras. Também utilizei como facilitador códigos implementados pelo openCV como circle() e o findcountors() para maior detalhamento da imagem nas bordas.

Peguei como exemplo esta imagem:

[[img-ironman]] 
.IronMan
image::ironman.png[]

A imagem sem o uso do algortimo de canny fica deste jeito:

[[img-ironmanP]] 
.IronManPontilhada
image::ironmanP.png[]

A imagem com Canny ficou desta maneira:

[[img-pontilhismo]] 
.Pontilhismo+Canny
image::pontilhismo.png[]